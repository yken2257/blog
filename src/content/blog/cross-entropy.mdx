---
title: "クロスエントロピー"
description: ""
pubDate: "July 23 2023"
---
import ExternalLink from '../../components/ExternalLink.astro';
export const components = {a: ExternalLink}

# ロジスティック回帰による二値分類
入力ベクトル$\boldsymbol{x}$に対して、出力$y$が0か1の二値であるような分類問題を考える。重みベクトル$\boldsymbol{w}$とバイアス$b$を用いて、
出力$y$が1である確率をロジスティック回帰によって次のようにモデル化する。

$$
q(y=1|\boldsymbol{x}) = \sigma(\boldsymbol{w}^T\boldsymbol{x} + b)
$$

ここで$\sigma(\cdot)$はシグモイド関数である。$q(y=1|\boldsymbol{x}_i)$は予測された確率分布で、データ$\boldsymbol{x}_i$が与えられたとき$y$が1となる確率を表すとする。
すると予測される出力$\hat{y}_i$は次のようになる。

$$
\hat{y}_i = \begin{cases}
1 & (q(y=1|\boldsymbol{x}_i) \geq 0.5) \\
0 & (q(y=1|\boldsymbol{x}_i) < 0.5)
\end{cases}
$$

これに対し、実際のデータに対する現実の確率分布$p$は以下のようになる。

$$
p(y|\boldsymbol{x}) = \begin{cases}
y_i & (y_i = 1) \\
1 - y_i & (y_i = 0)
\end{cases}
$$


データセット$\mathcal{D} = \{(\boldsymbol{x}_i, y_i)\}_{i=1}^N$があるとする。あるデータ$\boldsymbol{x}_i$について$y_i$が1である確率は$q(y=1|\boldsymbol{x}_i)$と予測され、$y_i$が0である確率は$q(y=0|\boldsymbol{x}_i)=1 - q(y=1|\boldsymbol{x}_i)$と予測される。
このとき、現実の値$y_i$が出力される確率は以下のようになる。

$$
\left [ q(\boldsymbol{x}_i) \right ]^{y_i} \left [1 - q(\boldsymbol{x}_i)\right ]^{(1-y_i)}
$$

データセット$\mathcal{D} = \{(\boldsymbol{x}_i, y_i)\}_{i=1}^N$と予測分布$q(\boldsymbol{x})$が与えられたとき、その予測分布が出力の組み合わせ$\{y_i\}_{i=1}^N$を実現する確率$L(\mathcal{D})$は次のようになる。

$$
\begin{align}
L(\mathcal{D}) &= \prod_{i=1}^N \left [ q(\boldsymbol{x}_i) \right ]^{y_i} \left [1 - q(\boldsymbol{x}_i)\right ]^{(1-y_i)} \\
&= \prod_{i=1}^N \left [ q(y=1|\boldsymbol{x}_i) \right ]^{p(y=1|\boldsymbol{x}_i)} \left [q(y=0|\boldsymbol{x}_i)\right ]^{p(y=0|\boldsymbol{x}_i)} \\
&= \prod_{i=1}^N \left [ q(y=y_i|\boldsymbol{x}_i) \right ]^{p(y=y_i|\boldsymbol{x}_i)}
\end{align}
$$

$L(\mathcal{D})$は尤度である。この尤度を最大化するようなパラメータ$\boldsymbol{w}, b$を求めることで、パラメータを推定することができる。
尤度の対数を取ると次のようになる。

$$
\begin{align}
\log L(\mathcal{D}) &= \sum_{i=1}^N \left\{ y_i \log q(\boldsymbol{x}_i) + (1-y_i) \log \left [1 - q(\boldsymbol{x}_i)\right ] \right\} \\
&= \sum_{i=1}^N \left\{ y_i \log \left [ \sigma\left (\boldsymbol{w}^T\boldsymbol{x}_i + b\right )\right ] + (1-y_i) \log \left [1 - \sigma\left (\boldsymbol{w}^T\boldsymbol{x}_i + b\right )\right ] \right\} \\
&= \sum_{i=1}^N \left\{ p(y=y_i|\boldsymbol{x}_i) \log q(y=y_i|\boldsymbol{x}_i) \right\}
\end{align}
$$

これを最大化する$\boldsymbol{w}, b$を求めることで、パラメータを推定することができる。

# クロスエントロピー

ある確率分布$p$と$q$が与えられたとき、その分布の間の距離を表す指標として、クロスエントロピーがある。クロスエントロピーは次のように定義される。

$$
H(p, q) = - \sum_{x} p(x) \log q(x)
$$

これは対数尤度の符号を反転させたものである。

# カルバック・ライブラー情報量

分布$p$と$q$の間の距離を表す指標として、カルバック・ライブラー情報量がある。カルバック・ライブラー情報量は次のように定義される。

$$
\begin{aligned}
D_\mathrm{KL}(p||q) &= \sum_{x} p(x) \log \frac{p(x)}{q(x)} \\
&= \sum_{x} p(x) \log p(x) - \sum_{x} p(x) \log q(x) \\
&= -H(p) + H(p, q)
\end{aligned}
$$

カルバック・ライブラー情報量は、分布$p$と$q$が近いほど小さくなる。また、分布$p$と$q$が同じであれば、カルバック・ライブラー情報量は最小値0をとる。